{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets import the modules and data we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88883, 85)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "#import util_functions as uf\n",
    "from util_functions import *\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('./2019/survey_results_public.csv')\n",
    "schema = pd.read_csv('./2019/survey_results_schema.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see if we can predict if someone describes their occupation as a Data Scientist/Machine Learning Specialist based on some of the colums from the data.  We have to be cautious what columns to select as the data will expload with columns if we attempt to use some that require one hot encoding.  These may or may not be the best columns to select; however, for simplicity and ease of training our model we will use them going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88883, 8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['EdLevel', 'UndergradMajor', 'Age', 'Hobbyist', 'DevType', 'WorkWeekHrs', 'WorkRemote', 'BetterLife']\n",
    "\n",
    "df = df[cols]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do some data manipulation in order to utilize the dataframe in a machine learning algorithm:\n",
    "\n",
    "1. Drop all the rows with no dev types\n",
    "2. For each numeric variable, fill the column with the mean value.\n",
    "3. Create y as the DevType column\n",
    "4. Set any value in y with 'Data scientist or machine learning specialist' to 1 otherwise 0\n",
    "5. Convert y column to an integer\n",
    "6. Create X to contain all columns excluding DevType\n",
    "6. Create dummy columns for all the categorical variables and drop the original columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y values equal to 1: 6460\n",
      "Shape of X: (81335, 29)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Drop rows with missing DevType values\n",
    "df = df[df.DevType.notnull()]\n",
    "\n",
    "# Fill numeric columns with the mean\n",
    "num_vars = df.select_dtypes(include=['float', 'int']).columns\n",
    "for col in num_vars:\n",
    "    df[col].fillna((df[col].mean()), inplace=True)\n",
    "\n",
    "# Set the y = 1 for any that have 'Data scientist or machine learning specialist'\n",
    "# in the string; otherwise, set y = 0\n",
    "y = df['DevType']\n",
    "\n",
    "for i, j in y.items():\n",
    "    if y[i].find('Data scientist or machine learning specialist') != -1:\n",
    "        y.at[i] = '1'       \n",
    "    else: \n",
    "        y.at[i] = '0'\n",
    "\n",
    "# Convert the series to integers\n",
    "y = y.astype('int32')\n",
    "\n",
    "# Lets get our X matrix by dropping the DevType column\n",
    "X = df.drop(['DevType'], axis=1)\n",
    "\n",
    "# Dummy the categorical variables\n",
    "cat_vars = X.select_dtypes(include=['object']).copy().columns\n",
    "for var in  cat_vars:\n",
    "    X = pd.concat([X.drop(var, axis=1), pd.get_dummies(X[var], prefix=var, prefix_sep='_', drop_first=True)], axis=1)\n",
    "\n",
    "# Make sure the number of rows that are related to a DevType of 'Data scientist or machine learning specialist' are 6460\n",
    "# and take a look at the X matrix as a sanity check that the columns didn't explode when using one hot encoding\n",
    "print('y values equal to 1: {}\\nShape of X: {}'.format(y.sum(), X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break up the X and y data into train and test batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65068, 29) (16267, 29) (65068,) (16267,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .20, random_state=101)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to try running AdaBoost with a Decision Tree as our algorithm with default parameter values to see what we can come up quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.89764554005\n",
      "0.10235445995\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "ada_model = AdaBoostClassifier(base_estimator = DecisionTreeClassifier()) \n",
    "\n",
    "ada_model.fit(X_train, y_train)\n",
    "y_test_preds = ada_model.predict(X_test)\n",
    "\n",
    "print(ada_model.score(X_test,y_test))\n",
    "print(mean_squared_error(y_test, y_test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use a grid search with a few different parameters to see if we can get any better results than the default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_estimator__criterion': 'entropy',\n",
       " 'base_estimator__max_depth': 2,\n",
       " 'base_estimator__splitter': 'best',\n",
       " 'n_estimators': 4}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n",
    "              \"base_estimator__max_depth\" :   [2, 4],\n",
    "              \"n_estimators\": [2,4,8,16,32]\n",
    "             }\n",
    "\n",
    "DTC = DecisionTreeClassifier()\n",
    "ABC = AdaBoostClassifier(base_estimator = DTC)\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(ABC, param_grid=param_grid)\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take the output from the previous grid search and plug in the one it states was the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.918669699391\n",
      "0.0813303006086\n"
     ]
    }
   ],
   "source": [
    "DTC = DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=2)\n",
    "ABC = AdaBoostClassifier(base_estimator=DTC, n_estimators=4)\n",
    "\n",
    "ABC.fit(X_train, y_train)\n",
    "y_test_preds = ABC.predict(X_test)\n",
    "\n",
    "print(ABC.score(X_test,y_test))\n",
    "print(mean_squared_error(y_test, y_test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got pretty good results tapping into the AdaBoost with a Decision Tree Classifier, but lets do a quick look at a SVC with default settings due to the length of time it takes to train SVC vs Ada+DT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.920022130694\n",
      "0.079977869306\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_model = SVC() \n",
    "svc_model.fit(X_train, y_train)\n",
    "y_test_preds = svc_model.predict(X_test) \n",
    "\n",
    "print(svc_model.score(X_test,y_test))\n",
    "print(mean_squared_error(y_test, y_test_preds)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see default AdaBoost with DTC wasn't horrible, but we did noticably better when doing a grid search of a subset of parameters.  What was interesing is default SVC was even better.  We potentially could get better doing a grid search on SVC; however, the time it takes to train that algorithm is substantially longer.  If time to train was a requirement in deciding the algorithm then using AdaBoost with DTC might be more beneficial; however, SVC seems to be a better solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
